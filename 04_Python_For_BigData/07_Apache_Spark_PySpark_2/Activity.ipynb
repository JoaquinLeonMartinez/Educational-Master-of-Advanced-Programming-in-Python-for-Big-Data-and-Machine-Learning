{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cbbe1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# IrisDataSet.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93a89710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/06/07 10:52:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/07 10:52:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('actividad08').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83bb19b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('IrisDataSet.csv', header=True).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "773c8572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
      "|  6|          5.4|         3.9|          1.7|         0.4|Iris-setosa|\n",
      "|  7|          4.6|         3.4|          1.4|         0.3|Iris-setosa|\n",
      "|  8|          5.0|         3.4|          1.5|         0.2|Iris-setosa|\n",
      "|  9|          4.4|         2.9|          1.4|         0.2|Iris-setosa|\n",
      "| 10|          4.9|         3.1|          1.5|         0.1|Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9280e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "529c6b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+-------------+------------+-----+\n",
      "|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|label|\n",
      "+-------------+------------+-------------+------------+-----+\n",
      "|          5.1|         3.5|          1.4|         0.2|  0.0|\n",
      "|          4.9|         3.0|          1.4|         0.2|  0.0|\n",
      "|          4.7|         3.2|          1.3|         0.2|  0.0|\n",
      "|          4.6|         3.1|          1.5|         0.2|  0.0|\n",
      "|          5.0|         3.6|          1.4|         0.2|  0.0|\n",
      "|          5.4|         3.9|          1.7|         0.4|  0.0|\n",
      "|          4.6|         3.4|          1.4|         0.3|  0.0|\n",
      "|          5.0|         3.4|          1.5|         0.2|  0.0|\n",
      "|          4.4|         2.9|          1.4|         0.2|  0.0|\n",
      "|          4.9|         3.1|          1.5|         0.1|  0.0|\n",
      "|          5.4|         3.7|          1.5|         0.2|  0.0|\n",
      "|          4.8|         3.4|          1.6|         0.2|  0.0|\n",
      "|          4.8|         3.0|          1.4|         0.1|  0.0|\n",
      "|          4.3|         3.0|          1.1|         0.1|  0.0|\n",
      "|          5.8|         4.0|          1.2|         0.2|  0.0|\n",
      "|          5.7|         4.4|          1.5|         0.4|  0.0|\n",
      "|          5.4|         3.9|          1.3|         0.4|  0.0|\n",
      "|          5.1|         3.5|          1.4|         0.3|  0.0|\n",
      "|          5.7|         3.8|          1.7|         0.3|  0.0|\n",
      "|          5.1|         3.8|          1.5|         0.3|  0.0|\n",
      "|          5.4|         3.4|          1.7|         0.2|  0.0|\n",
      "|          5.1|         3.7|          1.5|         0.4|  0.0|\n",
      "|          4.6|         3.6|          1.0|         0.2|  0.0|\n",
      "|          5.1|         3.3|          1.7|         0.5|  0.0|\n",
      "|          4.8|         3.4|          1.9|         0.2|  0.0|\n",
      "|          5.0|         3.0|          1.6|         0.2|  0.0|\n",
      "|          5.0|         3.4|          1.6|         0.4|  0.0|\n",
      "|          5.2|         3.5|          1.5|         0.2|  0.0|\n",
      "|          5.2|         3.4|          1.4|         0.2|  0.0|\n",
      "|          4.7|         3.2|          1.6|         0.2|  0.0|\n",
      "|          4.8|         3.1|          1.6|         0.2|  0.0|\n",
      "|          5.4|         3.4|          1.5|         0.4|  0.0|\n",
      "|          5.2|         4.1|          1.5|         0.1|  0.0|\n",
      "|          5.5|         4.2|          1.4|         0.2|  0.0|\n",
      "|          4.9|         3.1|          1.5|         0.1|  0.0|\n",
      "|          5.0|         3.2|          1.2|         0.2|  0.0|\n",
      "|          5.5|         3.5|          1.3|         0.2|  0.0|\n",
      "|          4.9|         3.1|          1.5|         0.1|  0.0|\n",
      "|          4.4|         3.0|          1.3|         0.2|  0.0|\n",
      "|          5.1|         3.4|          1.5|         0.2|  0.0|\n",
      "|          5.0|         3.5|          1.3|         0.3|  0.0|\n",
      "|          4.5|         2.3|          1.3|         0.3|  0.0|\n",
      "|          4.4|         3.2|          1.3|         0.2|  0.0|\n",
      "|          5.0|         3.5|          1.6|         0.6|  0.0|\n",
      "|          5.1|         3.8|          1.9|         0.4|  0.0|\n",
      "|          4.8|         3.0|          1.4|         0.3|  0.0|\n",
      "|          5.1|         3.8|          1.6|         0.2|  0.0|\n",
      "|          4.6|         3.2|          1.4|         0.2|  0.0|\n",
      "|          5.3|         3.7|          1.5|         0.2|  0.0|\n",
      "|          5.0|         3.3|          1.4|         0.2|  0.0|\n",
      "|          7.0|         3.2|          4.7|         1.4|  1.0|\n",
      "|          6.4|         3.2|          4.5|         1.5|  1.0|\n",
      "|          6.9|         3.1|          4.9|         1.5|  1.0|\n",
      "|          5.5|         2.3|          4.0|         1.3|  1.0|\n",
      "|          6.5|         2.8|          4.6|         1.5|  1.0|\n",
      "|          5.7|         2.8|          4.5|         1.3|  1.0|\n",
      "|          6.3|         3.3|          4.7|         1.6|  1.0|\n",
      "|          4.9|         2.4|          3.3|         1.0|  1.0|\n",
      "|          6.6|         2.9|          4.6|         1.3|  1.0|\n",
      "|          5.2|         2.7|          3.9|         1.4|  1.0|\n",
      "|          5.0|         2.0|          3.5|         1.0|  1.0|\n",
      "|          5.9|         3.0|          4.2|         1.5|  1.0|\n",
      "|          6.0|         2.2|          4.0|         1.0|  1.0|\n",
      "|          6.1|         2.9|          4.7|         1.4|  1.0|\n",
      "|          5.6|         2.9|          3.6|         1.3|  1.0|\n",
      "|          6.7|         3.1|          4.4|         1.4|  1.0|\n",
      "|          5.6|         3.0|          4.5|         1.5|  1.0|\n",
      "|          5.8|         2.7|          4.1|         1.0|  1.0|\n",
      "|          6.2|         2.2|          4.5|         1.5|  1.0|\n",
      "|          5.6|         2.5|          3.9|         1.1|  1.0|\n",
      "|          5.9|         3.2|          4.8|         1.8|  1.0|\n",
      "|          6.1|         2.8|          4.0|         1.3|  1.0|\n",
      "|          6.3|         2.5|          4.9|         1.5|  1.0|\n",
      "|          6.1|         2.8|          4.7|         1.2|  1.0|\n",
      "|          6.4|         2.9|          4.3|         1.3|  1.0|\n",
      "|          6.6|         3.0|          4.4|         1.4|  1.0|\n",
      "|          6.8|         2.8|          4.8|         1.4|  1.0|\n",
      "|          6.7|         3.0|          5.0|         1.7|  1.0|\n",
      "|          6.0|         2.9|          4.5|         1.5|  1.0|\n",
      "|          5.7|         2.6|          3.5|         1.0|  1.0|\n",
      "|          5.5|         2.4|          3.8|         1.1|  1.0|\n",
      "|          5.5|         2.4|          3.7|         1.0|  1.0|\n",
      "|          5.8|         2.7|          3.9|         1.2|  1.0|\n",
      "|          6.0|         2.7|          5.1|         1.6|  1.0|\n",
      "|          5.4|         3.0|          4.5|         1.5|  1.0|\n",
      "|          6.0|         3.4|          4.5|         1.6|  1.0|\n",
      "|          6.7|         3.1|          4.7|         1.5|  1.0|\n",
      "|          6.3|         2.3|          4.4|         1.3|  1.0|\n",
      "|          5.6|         3.0|          4.1|         1.3|  1.0|\n",
      "|          5.5|         2.5|          4.0|         1.3|  1.0|\n",
      "|          5.5|         2.6|          4.4|         1.2|  1.0|\n",
      "|          6.1|         3.0|          4.6|         1.4|  1.0|\n",
      "|          5.8|         2.6|          4.0|         1.2|  1.0|\n",
      "|          5.0|         2.3|          3.3|         1.0|  1.0|\n",
      "|          5.6|         2.7|          4.2|         1.3|  1.0|\n",
      "|          5.7|         3.0|          4.2|         1.2|  1.0|\n",
      "|          5.7|         2.9|          4.2|         1.3|  1.0|\n",
      "|          6.2|         2.9|          4.3|         1.3|  1.0|\n",
      "|          5.1|         2.5|          3.0|         1.1|  1.0|\n",
      "|          5.7|         2.8|          4.1|         1.3|  1.0|\n",
      "|          6.3|         3.3|          6.0|         2.5|  2.0|\n",
      "|          5.8|         2.7|          5.1|         1.9|  2.0|\n",
      "|          7.1|         3.0|          5.9|         2.1|  2.0|\n",
      "|          6.3|         2.9|          5.6|         1.8|  2.0|\n",
      "|          6.5|         3.0|          5.8|         2.2|  2.0|\n",
      "|          7.6|         3.0|          6.6|         2.1|  2.0|\n",
      "|          4.9|         2.5|          4.5|         1.7|  2.0|\n",
      "|          7.3|         2.9|          6.3|         1.8|  2.0|\n",
      "|          6.7|         2.5|          5.8|         1.8|  2.0|\n",
      "|          7.2|         3.6|          6.1|         2.5|  2.0|\n",
      "|          6.5|         3.2|          5.1|         2.0|  2.0|\n",
      "|          6.4|         2.7|          5.3|         1.9|  2.0|\n",
      "|          6.8|         3.0|          5.5|         2.1|  2.0|\n",
      "|          5.7|         2.5|          5.0|         2.0|  2.0|\n",
      "|          5.8|         2.8|          5.1|         2.4|  2.0|\n",
      "|          6.4|         3.2|          5.3|         2.3|  2.0|\n",
      "|          6.5|         3.0|          5.5|         1.8|  2.0|\n",
      "|          7.7|         3.8|          6.7|         2.2|  2.0|\n",
      "|          7.7|         2.6|          6.9|         2.3|  2.0|\n",
      "|          6.0|         2.2|          5.0|         1.5|  2.0|\n",
      "|          6.9|         3.2|          5.7|         2.3|  2.0|\n",
      "|          5.6|         2.8|          4.9|         2.0|  2.0|\n",
      "|          7.7|         2.8|          6.7|         2.0|  2.0|\n",
      "|          6.3|         2.7|          4.9|         1.8|  2.0|\n",
      "|          6.7|         3.3|          5.7|         2.1|  2.0|\n",
      "|          7.2|         3.2|          6.0|         1.8|  2.0|\n",
      "|          6.2|         2.8|          4.8|         1.8|  2.0|\n",
      "|          6.1|         3.0|          4.9|         1.8|  2.0|\n",
      "|          6.4|         2.8|          5.6|         2.1|  2.0|\n",
      "|          7.2|         3.0|          5.8|         1.6|  2.0|\n",
      "|          7.4|         2.8|          6.1|         1.9|  2.0|\n",
      "|          7.9|         3.8|          6.4|         2.0|  2.0|\n",
      "|          6.4|         2.8|          5.6|         2.2|  2.0|\n",
      "|          6.3|         2.8|          5.1|         1.5|  2.0|\n",
      "|          6.1|         2.6|          5.6|         1.4|  2.0|\n",
      "|          7.7|         3.0|          6.1|         2.3|  2.0|\n",
      "|          6.3|         3.4|          5.6|         2.4|  2.0|\n",
      "|          6.4|         3.1|          5.5|         1.8|  2.0|\n",
      "|          6.0|         3.0|          4.8|         1.8|  2.0|\n",
      "|          6.9|         3.1|          5.4|         2.1|  2.0|\n",
      "|          6.7|         3.1|          5.6|         2.4|  2.0|\n",
      "|          6.9|         3.1|          5.1|         2.3|  2.0|\n",
      "|          5.8|         2.7|          5.1|         1.9|  2.0|\n",
      "|          6.8|         3.2|          5.9|         2.3|  2.0|\n",
      "|          6.7|         3.3|          5.7|         2.5|  2.0|\n",
      "|          6.7|         3.0|          5.2|         2.3|  2.0|\n",
      "|          6.3|         2.5|          5.0|         1.9|  2.0|\n",
      "|          6.5|         3.0|          5.2|         2.0|  2.0|\n",
      "|          6.2|         3.4|          5.4|         2.3|  2.0|\n",
      "|          5.9|         3.0|          5.1|         1.8|  2.0|\n",
      "+-------------+------------+-------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# pasamos de sring a float los valores de input\n",
    "df = df.select(col('SepalLengthCm').cast('float'),\n",
    "                   col('SepalWidthCm').cast('float'),\n",
    "                   col('PetalLengthCm').cast('float'),\n",
    "                   col('PetalWidthCm').cast('float'),\n",
    "                    col('Species'))\n",
    "\n",
    "\n",
    "# Cambiamos el output a valores numericos -- 2 - Transformar los datos correspondientes a la columna especies usando StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"Species\", outputCol=\"label\")\n",
    "df = indexer.fit(df).transform(df)\n",
    "\n",
    "df = df.drop(\"Species\")\n",
    "\n",
    "df.show(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0944b2f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SepalLengthCm', 'float'),\n",
       " ('SepalWidthCm', 'float'),\n",
       " ('PetalLengthCm', 'float'),\n",
       " ('PetalWidthCm', 'float'),\n",
       " ('label', 'double')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e569875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+-------------+------------+-----+----------------------------------------------------------------------------+\n",
      "|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|label|Features                                                                    |\n",
      "+-------------+------------+-------------+------------+-----+----------------------------------------------------------------------------+\n",
      "|5.1          |3.5         |1.4          |0.2         |0.0  |[5.099999904632568,3.5,1.399999976158142,0.20000000298023224]               |\n",
      "|4.9          |3.0         |1.4          |0.2         |0.0  |[4.900000095367432,3.0,1.399999976158142,0.20000000298023224]               |\n",
      "|4.7          |3.2         |1.3          |0.2         |0.0  |[4.699999809265137,3.200000047683716,1.2999999523162842,0.20000000298023224]|\n",
      "|4.6          |3.1         |1.5          |0.2         |0.0  |[4.599999904632568,3.0999999046325684,1.5,0.20000000298023224]              |\n",
      "|5.0          |3.6         |1.4          |0.2         |0.0  |[5.0,3.5999999046325684,1.399999976158142,0.20000000298023224]              |\n",
      "|5.4          |3.9         |1.7          |0.4         |0.0  |[5.400000095367432,3.9000000953674316,1.7000000476837158,0.4000000059604645]|\n",
      "|4.6          |3.4         |1.4          |0.3         |0.0  |[4.599999904632568,3.4000000953674316,1.399999976158142,0.30000001192092896]|\n",
      "|5.0          |3.4         |1.5          |0.2         |0.0  |[5.0,3.4000000953674316,1.5,0.20000000298023224]                            |\n",
      "|4.4          |2.9         |1.4          |0.2         |0.0  |[4.400000095367432,2.9000000953674316,1.399999976158142,0.20000000298023224]|\n",
      "|4.9          |3.1         |1.5          |0.1         |0.0  |[4.900000095367432,3.0999999046325684,1.5,0.10000000149011612]              |\n",
      "|5.4          |3.7         |1.5          |0.2         |0.0  |[5.400000095367432,3.700000047683716,1.5,0.20000000298023224]               |\n",
      "|4.8          |3.4         |1.6          |0.2         |0.0  |[4.800000190734863,3.4000000953674316,1.600000023841858,0.20000000298023224]|\n",
      "|4.8          |3.0         |1.4          |0.1         |0.0  |[4.800000190734863,3.0,1.399999976158142,0.10000000149011612]               |\n",
      "|4.3          |3.0         |1.1          |0.1         |0.0  |[4.300000190734863,3.0,1.100000023841858,0.10000000149011612]               |\n",
      "|5.8          |4.0         |1.2          |0.2         |0.0  |[5.800000190734863,4.0,1.2000000476837158,0.20000000298023224]              |\n",
      "|5.7          |4.4         |1.5          |0.4         |0.0  |[5.699999809265137,4.400000095367432,1.5,0.4000000059604645]                |\n",
      "|5.4          |3.9         |1.3          |0.4         |0.0  |[5.400000095367432,3.9000000953674316,1.2999999523162842,0.4000000059604645]|\n",
      "|5.1          |3.5         |1.4          |0.3         |0.0  |[5.099999904632568,3.5,1.399999976158142,0.30000001192092896]               |\n",
      "|5.7          |3.8         |1.7          |0.3         |0.0  |[5.699999809265137,3.799999952316284,1.7000000476837158,0.30000001192092896]|\n",
      "|5.1          |3.8         |1.5          |0.3         |0.0  |[5.099999904632568,3.799999952316284,1.5,0.30000001192092896]               |\n",
      "+-------------+------------+-------------+------------+-----+----------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Utilizamos el assembler 1 -- Transformar en vector los datos numéricos usando VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"],\n",
    "    outputCol=\"Features\"\n",
    ")\n",
    "\n",
    "output = assembler.transform(df)\n",
    "\n",
    "output.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21f770b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+-------------+------------+-------+--------------------+\n",
      "|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|Species|            Features|\n",
      "+-------------+------------+-------------+------------+-------+--------------------+\n",
      "|          5.1|         3.5|          1.4|         0.2|    0.0|[5.09999990463256...|\n",
      "|          4.9|         3.0|          1.4|         0.2|    0.0|[4.90000009536743...|\n",
      "|          4.7|         3.2|          1.3|         0.2|    0.0|[4.69999980926513...|\n",
      "|          4.6|         3.1|          1.5|         0.2|    0.0|[4.59999990463256...|\n",
      "|          5.0|         3.6|          1.4|         0.2|    0.0|[5.0,3.5999999046...|\n",
      "|          5.4|         3.9|          1.7|         0.4|    0.0|[5.40000009536743...|\n",
      "|          4.6|         3.4|          1.4|         0.3|    0.0|[4.59999990463256...|\n",
      "|          5.0|         3.4|          1.5|         0.2|    0.0|[5.0,3.4000000953...|\n",
      "|          4.4|         2.9|          1.4|         0.2|    0.0|[4.40000009536743...|\n",
      "|          4.9|         3.1|          1.5|         0.1|    0.0|[4.90000009536743...|\n",
      "|          5.4|         3.7|          1.5|         0.2|    0.0|[5.40000009536743...|\n",
      "|          4.8|         3.4|          1.6|         0.2|    0.0|[4.80000019073486...|\n",
      "|          4.8|         3.0|          1.4|         0.1|    0.0|[4.80000019073486...|\n",
      "|          4.3|         3.0|          1.1|         0.1|    0.0|[4.30000019073486...|\n",
      "|          5.8|         4.0|          1.2|         0.2|    0.0|[5.80000019073486...|\n",
      "|          5.7|         4.4|          1.5|         0.4|    0.0|[5.69999980926513...|\n",
      "|          5.4|         3.9|          1.3|         0.4|    0.0|[5.40000009536743...|\n",
      "|          5.1|         3.5|          1.4|         0.3|    0.0|[5.09999990463256...|\n",
      "|          5.7|         3.8|          1.7|         0.3|    0.0|[5.69999980926513...|\n",
      "|          5.1|         3.8|          1.5|         0.3|    0.0|[5.09999990463256...|\n",
      "+-------------+------------+-------------+------------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos con las predicciones\n",
    "\n",
    "# Lo primero es renombrar la columna label y volvemos a la variable df\n",
    "df = output.withColumnRenamed(\"label\", \"Species\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51c652d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos el dataframe para las predicciones de despues:\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ef50b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+-------------+------------+-------+--------------------+\n",
      "|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|Species|            Features|\n",
      "+-------------+------------+-------------+------------+-------+--------------------+\n",
      "|          4.3|         3.0|          1.1|         0.1|    0.0|[4.30000019073486...|\n",
      "|          4.4|         2.9|          1.4|         0.2|    0.0|[4.40000009536743...|\n",
      "|          4.4|         3.2|          1.3|         0.2|    0.0|[4.40000009536743...|\n",
      "|          4.5|         2.3|          1.3|         0.3|    0.0|[4.5,2.2999999523...|\n",
      "|          4.6|         3.1|          1.5|         0.2|    0.0|[4.59999990463256...|\n",
      "|          4.6|         3.4|          1.4|         0.3|    0.0|[4.59999990463256...|\n",
      "|          4.6|         3.6|          1.0|         0.2|    0.0|[4.59999990463256...|\n",
      "|          4.7|         3.2|          1.3|         0.2|    0.0|[4.69999980926513...|\n",
      "|          4.7|         3.2|          1.6|         0.2|    0.0|[4.69999980926513...|\n",
      "|          4.8|         3.0|          1.4|         0.1|    0.0|[4.80000019073486...|\n",
      "|          4.8|         3.4|          1.6|         0.2|    0.0|[4.80000019073486...|\n",
      "|          4.8|         3.4|          1.9|         0.2|    0.0|[4.80000019073486...|\n",
      "|          4.9|         2.4|          3.3|         1.0|    1.0|[4.90000009536743...|\n",
      "|          4.9|         2.5|          4.5|         1.7|    2.0|[4.90000009536743...|\n",
      "|          4.9|         3.1|          1.5|         0.1|    0.0|[4.90000009536743...|\n",
      "|          4.9|         3.1|          1.5|         0.1|    0.0|[4.90000009536743...|\n",
      "|          4.9|         3.1|          1.5|         0.1|    0.0|[4.90000009536743...|\n",
      "|          5.0|         2.0|          3.5|         1.0|    1.0|   [5.0,2.0,3.5,1.0]|\n",
      "|          5.0|         3.0|          1.6|         0.2|    0.0|[5.0,3.0,1.600000...|\n",
      "|          5.0|         3.2|          1.2|         0.2|    0.0|[5.0,3.2000000476...|\n",
      "+-------------+------------+-------------+------------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 - Tree Classifier\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol='Species',\n",
    "                           featuresCol='Features',\n",
    "                           maxDepth=5)\n",
    "\n",
    "train_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0e5f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTC = dt.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96d290ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+-------------+------------+-------+--------------------+--------------+-------------+----------+\n",
      "|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|Species|            Features| rawPrediction|  probability|prediction|\n",
      "+-------------+------------+-------------+------------+-------+--------------------+--------------+-------------+----------+\n",
      "|          4.4|         3.0|          1.3|         0.2|    0.0|[4.40000009536743...|[37.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|          4.6|         3.2|          1.4|         0.2|    0.0|[4.59999990463256...|[37.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|          4.8|         3.0|          1.4|         0.3|    0.0|[4.80000019073486...|[37.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|          4.8|         3.1|          1.6|         0.2|    0.0|[4.80000019073486...|[37.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|          4.9|         3.0|          1.4|         0.2|    0.0|[4.90000009536743...|[37.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "+-------------+------------+-------------+------------+-------+--------------------+--------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = modelTC.transform(test_df)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb2535d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy =  0.9310344827586207\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Species',\n",
    "                                              predictionCol='prediction',\n",
    "                                              metricName='accuracy')\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print('Test Accuracy = ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d357a64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Gradient-boosted tree classifier\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(labelCol=\"Species\", featuresCol=\"Features\", maxIter=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "821059b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/07 10:57:55 ERROR Executor: Exception in task 0.0 in stage 26.0 (TID 26)\n",
      "java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:177)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:174)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1207)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2290)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/07 10:57:55 WARN TaskSetManager: Lost task 0.0 in stage 26.0 (TID 26) (e8614ac1fc66 executor driver): java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:177)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:174)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1207)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2290)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/06/07 10:57:55 ERROR TaskSetManager: Task 0 in stage 26.0 failed 1 times; aborting job\n",
      "23/06/07 10:57:55 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 26) (e8614ac1fc66 executor driver): java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:177)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:174)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1207)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2290)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1209)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1202)\n",
      "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n",
      "\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:333)\n",
      "\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:61)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$1(GBTClassifier.scala:210)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:171)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:59)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:177)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:174)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1207)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2290)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o368.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 26) (e8614ac1fc66 executor driver): java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:177)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:174)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1207)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2290)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1209)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1202)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:333)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:61)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$1(GBTClassifier.scala:210)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:171)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:59)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:177)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:174)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1207)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2290)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4130/1840504172.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodelGBT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o368.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 26) (e8614ac1fc66 executor driver): java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:177)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:174)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1207)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2290)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1209)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1202)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:333)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:61)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$1(GBTClassifier.scala:210)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:171)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:59)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:177)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:174)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1207)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2290)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "modelGBT = gbt.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6c8893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El error indica que \"GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification\"\n",
    "\n",
    "# Es decir, que al no ser una clasificacion binaria no se puede utilizar. (ya que buscamos tres posibles salidas, los tres tipos de flores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19908e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 - Random Forest Classifier\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15625e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"Species\", featuresCol=\"Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1a355a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRF = rf.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55ce2f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+-------------+------------+-------+--------------------+--------------+-------------+----------+\n",
      "|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|Species|            Features| rawPrediction|  probability|prediction|\n",
      "+-------------+------------+-------------+------------+-------+--------------------+--------------+-------------+----------+\n",
      "|          4.4|         3.0|          1.3|         0.2|    0.0|[4.40000009536743...|[20.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|          4.6|         3.2|          1.4|         0.2|    0.0|[4.59999990463256...|[20.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|          4.8|         3.0|          1.4|         0.3|    0.0|[4.80000019073486...|[20.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|          4.8|         3.1|          1.6|         0.2|    0.0|[4.80000019073486...|[20.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|          4.9|         3.0|          1.4|         0.2|    0.0|[4.90000009536743...|[20.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "+-------------+------------+-------------+------------+-------+--------------------+--------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = modelRF.transform(test_df)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60b92ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy =  0.9655172413793104\n"
     ]
    }
   ],
   "source": [
    "evaluatorRF = MulticlassClassificationEvaluator(labelCol='Species',\n",
    "                                              predictionCol='prediction',\n",
    "                                              metricName='accuracy')\n",
    "\n",
    "accuracyRF = evaluatorRF.evaluate(predictions)\n",
    "\n",
    "print('Test Accuracy = ', accuracyRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fac06e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
