{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49a9e6db",
   "metadata": {},
   "source": [
    "# Actividad 02 Joaquin Leon Martinez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57c8704",
   "metadata": {},
   "source": [
    "## 1. Dado un conjunto de 30 datos de test, con la variable objetivo real y la salida proporcionada por dos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe31c8a",
   "metadata": {},
   "source": [
    "### 1.1 Calcular las métricas de regresión para cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd7c387b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y objetivo</th>\n",
       "      <th>Predicciones M1</th>\n",
       "      <th>Predicciones M2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.50</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.00</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.60</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.00</td>\n",
       "      <td>8.1</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.56</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Y objetivo  Predicciones M1  Predicciones M2\n",
       "0        2.50              3.0              2.0\n",
       "1        3.00              2.9              2.0\n",
       "2        1.60              2.0              2.0\n",
       "3        8.00              8.1              7.0\n",
       "4        4.56              4.0              5.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "# Lo primero es cargar los datos\n",
    "df1 = pd.read_csv(\"l2p1.csv\")\n",
    "\n",
    "# Mostramos\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4e67f",
   "metadata": {},
   "source": [
    "#### 1.1.1  MAE (Error Absoluto Medio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852decfa",
   "metadata": {},
   "source": [
    "Es la diferencia entre el valor pronosticado y el valor real en cada punto pronosticado, conceptualmente la métrica de evaluación más fácil para problemas de regresión. Responde a la pregunta: \"¿En qué medida te equivocaste en tus predicciones, en promedio?\"\n",
    "\n",
    "\n",
    "Para comprenderlo mejor tomemos el siguiente ejemplo: consideremos que estamos pronosticando los precios de vehículos usados. El precio real de un vehículo usado es de 10k. Después de aplicar un modelo de Machine Learning predijimos que el precio es de 12k, ahora podemos observar que la diferencia es de 2k entre ambos precios de venta. Esta diferencia se llama error absoluto. \n",
    "\n",
    "\n",
    "(Δ y) = | y i — ŷ |\n",
    "\n",
    "y i : representa el valor real\n",
    "\n",
    "ŷ : representa el valor predicho\n",
    "\n",
    "El caso anterior representa el error absoluto para una sola observación. Pero en la práctica tendremos muchos datos para predecir la salida. Por lo tanto, usamos para obtener el error absoluto medio (MAE), es decir, la media de los errores absolutos de toda la observación.\n",
    "\n",
    "\n",
    "Ventajas de MAE:\n",
    "\n",
    "- Es una métrica de evaluación fácil de calcular.\n",
    "- Todos los errores se ponderan en la misma escala ya que se toman valores absolutos.\n",
    "- Es útil si los datos de entrenamiento tienen valores atípicos, ya que MAE no penaliza los errores elevados causados por los valores atípicos.\n",
    "- Proporciona una medida uniforme de qué tan bien está funcionando el modelo.\n",
    "\n",
    "\n",
    "Desventajas de MAE\n",
    "\n",
    "- MAE sigue una medida de precisión dependiente de la escala en la que utiliza la misma escala que los datos que se están midiendo. Por lo tanto, no puede usarse para comparar series usando diferentes medidas.\n",
    "- Una de las principales desventajas de MAE es que no es diferenciable en cero. Muchos algoritmos de optimización tienden a utilizar la diferenciación para encontrar el valor óptimo de los parámetros en la métrica de evaluación.\n",
    "- Puede ser un desafío calcular gradientes en MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26cb7b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE calculado por nosotros:  1.0396666666666667\n",
      "MAE calculado por pandas:  1.0396666666666667\n"
     ]
    }
   ],
   "source": [
    "# Procedemos a calcular MAE:\n",
    "\n",
    "# MAE para el Modelo 1\n",
    "# Necesitamos obtener el numero de elementos que hay en el df1\n",
    "num_elementos = df1.shape[0]\n",
    "#print(num_elementos)\n",
    "\n",
    "# Calcular la diferencia en valor absoluto entre todos los valores de la primera y la segunda columna\n",
    "dif_abs = (df1['Y objetivo'] - df1['Predicciones M1']).abs()\n",
    "#print(dif_abs)\n",
    "\n",
    "# Calculamos el MAE\n",
    "MAE_M1 = dif_abs.sum()/num_elementos\n",
    "\n",
    "# Comparamos con el que calcula pandas\n",
    "MAE_M1_PD = mean_absolute_error(df1['Y objetivo'], df1['Predicciones M1'])\n",
    "\n",
    "print(\"MAE calculado por nosotros: \", MAE_M1 )\n",
    "print(\"MAE calculado por pandas: \", MAE_M1_PD )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf20fc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE calculado por nosotros:  0.6113333333333333\n",
      "MAE calculado por pandas:  0.6113333333333333\n"
     ]
    }
   ],
   "source": [
    "# MAE para el Modelo 2\n",
    "# Calcular la diferencia en valor absoluto entre la primera y la tercera columna\n",
    "dif_abs2 = (df1['Y objetivo'] - df1['Predicciones M2']).abs()\n",
    "#print(dif_abs)\n",
    "\n",
    "# Calculamos el MAE\n",
    "MAE_M2 = dif_abs2.sum()/num_elementos\n",
    "\n",
    "# Comparamos con el que calcula pandas\n",
    "MAE_M2_PD = mean_absolute_error(df1['Y objetivo'], df1['Predicciones M2'])\n",
    "\n",
    "print(\"MAE calculado por nosotros: \", MAE_M2 )\n",
    "print(\"MAE calculado por pandas: \", MAE_M2_PD )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184e15f9",
   "metadata": {},
   "source": [
    "#### 1.1.2 MSE (Error cuadrático medio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6ec1ba",
   "metadata": {},
   "source": [
    "MSE básicamente mide el error cuadrado promedio de nuestras predicciones. Para cada punto, calcula la diferencia cuadrada entre las predicciones y el objetivo y luego promedia esos valores.\n",
    "\n",
    "Cuanto mayor sea este valor, peor es el modelo. Nunca es negativo, ya que estamos cuadrando los errores de predicción individuales antes de sumarlos, pero sería cero para un modelo perfecto.\n",
    "\n",
    "Ventaja:\n",
    "- Útil si tenemos valores inesperados que nos deberían interesar. Muy alto o bajo valor que debemos prestar atención.\n",
    "\n",
    "Desventajas:\n",
    "- Si hacemos una predicción muy mala, la cuadratura empeorará aún más el error y puede sesgar la métrica para sobreestimar la maldad del modelo. Este es un comportamiento particularmente problemático si tenemos datos ruidosos (es decir, los datos que por cualquier motivo no son del todo confiables).\n",
    "- Incluso un modelo “perfecto” puede tener un MSE alto en esa situación. Por lo que es difícil juzgar qué tan bien modelo está realizando.\n",
    "- Por otro lado, si todos los errores son pequeños, o más bien, más pequeños que 1, se siente el efecto contrario: podemos subestimar la maldad del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69be602a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE calculado por nosotros:  7.9830499999999995\n",
      "MSE calculado por pandas:  7.9830499999999995\n"
     ]
    }
   ],
   "source": [
    "# Procedemos a calcular MSE:\n",
    "\n",
    "# MSE para el Modelo 1\n",
    "# Necesitamos obtener el numero de elementos que hay en el df1\n",
    "num_elementos = df1.shape[0]\n",
    "#print(num_elementos)\n",
    "\n",
    "# diferencia en valor absoluto entre todos los valores de la primera y la segunda columna Y ESTA VEZ SE ELEVA AL CUADRADO\n",
    "dif_abs = (df1['Y objetivo'] - df1['Predicciones M1'])**2\n",
    "#print(dif_abs)\n",
    "\n",
    "# Calculamos el MAE\n",
    "MSE_M1 = dif_abs.sum()/num_elementos\n",
    "\n",
    "# Comparamos con el que calcula pandas\n",
    "MSE_M1_PD = mean_squared_error(df1['Y objetivo'], df1['Predicciones M1'])\n",
    "\n",
    "print(\"MSE calculado por nosotros: \", MSE_M1 )\n",
    "print(\"MSE calculado por pandas: \", MSE_M1_PD )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eae6a456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE calculado por nosotros:  0.5219533333333334\n",
      "MSE calculado por pandas:  0.5219533333333334\n"
     ]
    }
   ],
   "source": [
    "MSE_M2 = ((df1['Y objetivo'] - df1['Predicciones M2'])**2).sum()/num_elementos\n",
    "\n",
    "# Comparamos con el que calcula pandas\n",
    "MSE_M2_PD = mean_squared_error(df1['Y objetivo'], df1['Predicciones M2'])\n",
    "\n",
    "print(\"MSE calculado por nosotros: \", MSE_M2 )\n",
    "print(\"MSE calculado por pandas: \", MSE_M2_PD )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a23916",
   "metadata": {},
   "source": [
    "#### 1.1.2 RMSE (Root Mean Squared Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555abe45",
   "metadata": {},
   "source": [
    "RMSE es solo la raíz cuadrada de MSE. La raíz cuadrada se introduce para hacer que la escala de los errores sea igual a la escala de los objetivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56ac77e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE_M1 calculado:  2.8254291709402306\n"
     ]
    }
   ],
   "source": [
    "# Procedemos a calcular el RMSE\n",
    "\n",
    "RMSE_M1 = math.sqrt(MSE_M1)\n",
    "\n",
    "print(\"RMSE_M1 calculado: \", RMSE_M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a88871a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE_M2 calculado:  0.7224633785413164\n"
     ]
    }
   ],
   "source": [
    "# Lo mismo con el segundo mmodelo:\n",
    "RMSE_M2 = math.sqrt(MSE_M2)\n",
    "\n",
    "print(\"RMSE_M2 calculado: \", RMSE_M2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b6fab",
   "metadata": {},
   "source": [
    "### 1.2 En función de los resultados, decidir qué modelo es mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f3389e",
   "metadata": {},
   "source": [
    "Basandonos en que todas las metricas de regresión calculadas tienen valores menores para el modelo 2, podemos afirmar que el segundo modelo es mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ae90d",
   "metadata": {},
   "source": [
    "## 2. Dado un problema de clasificación binaria (sólo existen dos clases), tenemos la salida de cada uno de 30 patrones en test para dos modelos distintos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf4443c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e794c5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clase Objetivo</th>\n",
       "      <th>Predicciones M1</th>\n",
       "      <th>Predicciones M2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clase Objetivo  Predicciones M1  Predicciones M2\n",
       "0               0                1                0\n",
       "1               0                0                0\n",
       "2               0                0                0\n",
       "3               1                1                1\n",
       "4               1                1                1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lo primero es cargar los datos\n",
    "df2 = pd.read_csv(\"l2p2.csv\")\n",
    "\n",
    "# Mostramos\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c0eb71",
   "metadata": {},
   "source": [
    "### 2.1 Montar la matriz de confusión de cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ce7ac24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión calculada:\n",
      "[[19  1]\n",
      " [ 4  6]]\n",
      "Matriz de confusión de pandas:\n",
      "[[19  1]\n",
      " [ 4  6]]\n"
     ]
    }
   ],
   "source": [
    "# Calcular la matriz de confusión:\n",
    "\n",
    "# Calculammos TP (True positives)\n",
    "TP1 = ((df2['Clase Objetivo'] == 0) & (df2['Predicciones M1'] == 0)).sum()\n",
    "# Calculammos FN (False negatives)\n",
    "FN1 = ((df2['Clase Objetivo'] == 0) & (df2['Predicciones M1'] == 1)).sum()\n",
    "# Calculammos FP (False positives)\n",
    "FP1 = ((df2['Clase Objetivo'] == 1) & (df2['Predicciones M1'] == 0)).sum()\n",
    "# Calculammos TN (True negatives)\n",
    "TN1 = ((df2['Clase Objetivo'] == 1) & (df2['Predicciones M1'] == 1)).sum()\n",
    "\n",
    "# Montamos la matriz\n",
    "CM1 = np.array([[TP1, FN1], \n",
    "       [FP1, TN1]])\n",
    "\n",
    "# La calculamos tambbien con la libreria:\n",
    "clases = df2['Clase Objetivo'].unique()\n",
    "CM1_PD = confusion_matrix(df2['Clase Objetivo'], df2['Predicciones M1'], labels=clases)\n",
    "\n",
    "# Mostramos resultados\n",
    "print(\"Matriz de confusión calculada:\")\n",
    "print(CM1)\n",
    "\n",
    "print(\"Matriz de confusión de pandas:\")\n",
    "print(CM1_PD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ada66a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión calculada:\n",
      "[[12  8]\n",
      " [ 1  9]]\n",
      "Matriz de confusión de pandas:\n",
      "[[12  8]\n",
      " [ 1  9]]\n"
     ]
    }
   ],
   "source": [
    "# Calcular la matriz de confusión para el segundo modelo\n",
    "\n",
    "# Calculammos TP (True positives)\n",
    "TP2 = ((df2['Clase Objetivo'] == 0) & (df2['Predicciones M2'] == 0)).sum()\n",
    "# Calculammos FN (False negatives)\n",
    "FN2 = ((df2['Clase Objetivo'] == 0) & (df2['Predicciones M2'] == 1)).sum()\n",
    "# Calculammos FP (False positives)\n",
    "FP2 = ((df2['Clase Objetivo'] == 1) & (df2['Predicciones M2'] == 0)).sum()\n",
    "# Calculammos TN (True negatives)\n",
    "TN2 = ((df2['Clase Objetivo'] == 1) & (df2['Predicciones M2'] == 1)).sum()\n",
    "\n",
    "# Montamos la matriz\n",
    "CM2 = np.array([[TP2, FN2], \n",
    "       [FP2, TN2]])\n",
    "\n",
    "# La calculamos tambbien con la libreria:\n",
    "clases = df2['Clase Objetivo'].unique()\n",
    "CM2_PD = confusion_matrix(df2['Clase Objetivo'], df2['Predicciones M2'], labels=clases)\n",
    "\n",
    "# Mostramos resultados\n",
    "print(\"Matriz de confusión calculada:\")\n",
    "print(CM2)\n",
    "\n",
    "print(\"Matriz de confusión de pandas:\")\n",
    "print(CM2_PD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752fd617",
   "metadata": {},
   "source": [
    "### 2.2 Calcular las métricas de clasificación, excepto el AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf8ed6",
   "metadata": {},
   "source": [
    "#### 2.2.1 CCR (Precision Global): Porcentaje de patrones correctamente clasificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26712ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# CCR Modelo 1\n",
    "\n",
    "# Numero total de elementos:\n",
    "N = df2.shape[0]\n",
    "\n",
    "CCR_M1 = (TP1 + TN1)/N\n",
    "\n",
    "print(CCR_M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ec42b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    }
   ],
   "source": [
    "# CCR Modelo 2\n",
    "\n",
    "CCR_M2 = (TP2 + TN2)/N\n",
    "\n",
    "print(CCR_M2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4cb85",
   "metadata": {},
   "source": [
    "#### 2.2.2 Sensibilidad (Recall o TP Rate): Porcentaje de patrones positivos predichos como positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ca4b23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n"
     ]
    }
   ],
   "source": [
    "# Sensibilidad Modelo 1\n",
    "\n",
    "TPR_M1 = TP1/(TP1+FN1)\n",
    "\n",
    "print(TPR_M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c2229b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "# Sensibilidad Modelo 2\n",
    "\n",
    "TPR_M2 = TP2/(TP2+FN2)\n",
    "\n",
    "print(TPR_M2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6901c06d",
   "metadata": {},
   "source": [
    "#### 2.2.3 False Positive Rate (FP Rate): Porcentaje de patrones negativos predichos como positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "caf42985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "# FPR Modelo 1\n",
    "\n",
    "FPR_M1 = FP1/(TN1+FP1)\n",
    "\n",
    "print(FPR_M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3f09c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "# FPR Modelo 2\n",
    "\n",
    "FPR_M2 = FP2/(TN2+FP2)\n",
    "\n",
    "print(FPR_M2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf661765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8260869565217391\n"
     ]
    }
   ],
   "source": [
    "# Precision Modelo 1\n",
    "\n",
    "ACC_M1 = TP1/(TP1+FP1)\n",
    "\n",
    "print(ACC_M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ced8e8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "# Precision Modelo 2\n",
    "\n",
    "ACC_M2 = TP2/(TP2+FP2)\n",
    "\n",
    "print(ACC_M2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3fcf540a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8837209302325583\n"
     ]
    }
   ],
   "source": [
    "# F1-Score Modelo 1\n",
    "\n",
    "F1Score_M1 = (2*ACC_M1*TPR_M1)/(ACC_M1+TPR_M1)\n",
    "print(F1Score_M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "226b167c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7272727272727274\n"
     ]
    }
   ],
   "source": [
    "# F1-Score Modelo 2\n",
    "\n",
    "F1Score_M2 = (2*ACC_M2*TPR_M2)/(ACC_M2+TPR_M2)\n",
    "print(F1Score_M2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230e349f",
   "metadata": {},
   "source": [
    "### 2.3 Determinar qué modelo es mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb07d96",
   "metadata": {},
   "source": [
    "En base a los datos obtenidos podemos determinar que el modelo 2 es mejor que el modelo 1, en este caso no hay una diferencia tan significativa como en el ejercicio anterior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
